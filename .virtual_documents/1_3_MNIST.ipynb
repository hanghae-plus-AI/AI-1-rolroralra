


import torch
import torchvision
import torchvision.transforms as transforms

# 디바이스 설정
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("CUDA 장치 사용 가능")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("MPS 장치 사용 가능")
else:
    device = torch.device("cpu")
    print("MPS와 CUDA 모두 불가능, CPU 사용")

# 디바이스 출력
print(f"사용중인 디바이스: {device}")





transform = transforms.ToTensor()

trainset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)





from matplotlib import pyplot as plt


print(len(trainset))
print(trainset[0][0].shape, trainset[0][1])
plt.imshow(trainset[0][0][0], cmap='gray')





batch_size = 64

trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=batch_size,
    shuffle=True
)





dataiter = iter(trainloader)
images, labels = next(dataiter)
print(images.shape, labels.shape)





from torch import nn


class Model(nn.Module):
  def __init__(self, input_dim, n_dim):
    super().__init__()

    self.layer1 = nn.Linear(input_dim, n_dim)
    self.layer2 = nn.Linear(n_dim, n_dim)
    self.layer3 = nn.Linear(n_dim, 1)

    self.act = nn.ReLU()

  def forward(self, x):
    x = torch.flatten(x, start_dim=1)
    x = self.act(self.layer1(x))
    x = self.act(self.layer2(x))
    x = self.act(self.layer3(x))

    return x


model = Model(28 * 28 * 1, 1024)





from torch.optim import SGD

lr = 0.001
model = model.to(device)

optimizer = SGD(model.parameters(), lr=lr)





n_epochs = 100

for epoch in range(n_epochs):
  total_loss = 0.
  for data in trainloader:
    model.zero_grad()
    inputs, labels = data
    inputs, labels = inputs.to(device), labels.to(device)

    preds = model(inputs)
    loss = (preds[:, 0] - labels).pow(2).mean()
    loss.backward()
    optimizer.step()

    total_loss += loss.item()

  print(f"Epoch {epoch:3d} | Loss: {total_loss}")





idx = 0

x = trainset[idx][0][None]  # (1, 1, 28, 28)
x = x.to(device)

print(model(x))
print(trainset[idx][1])






